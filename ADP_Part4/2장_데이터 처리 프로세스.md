# 2장. 데이터 처리 기술
## 학습목표
    - 분산 파일 시스템, 공유 스토리지 등의 저장 기술의 종류와 기능을 이해한다
    - 분산 병렬 처리 기술에 대해 이해한다
    - 서버 가상화를 중심으로 한 클라우드 인프라 기술들을 이해한다
<hr>

## 1절. 분산 데이터 저장 기술
### 1. 분산 파일 시스템
#### 가. 분산 파일 시스템의 개요
    - 분산 파일시스템, 클러스터, 데이터베이스, NoSQL로 구분
    - 파일의 메타데이터를 관리하는 전용 서버를 가지고 있는 비대칭형 클러스터 파일 시스템 개발
    - 메타데이터에 접근하는 경로와 데이터에 접근하는 경로가 분리된 구조
#### 나. 구글 파일 시스템(GFS, Google File System)
##### 1) 개념 및 특징
    - 파일을 고정된 크기의 청크로 나누고, 각 청크에 대한 여러 개의 복제본과 청크를 청크서버에 분산 저장한다
    - 트리 구조가 아닌 해시 테이블 구조 등을 사용해서 메모리상 효율적
    - 청크는 마스터에 의해 생성/삭제 가능, 유일한 식별자에 의해 구별
##### 2) GFS 설계의 가정
    - 서버의 고장이 빈번할 수 있다.
    - 대부분의 파일은 대용량
    - 작업의 부하는 주로 연속적으로 많은 데이터를 읽는 연산 또는 임의의 영역에서 적은 데이터를 읽는 연산에서 발생
    - 쓰기연산은 순차적, 파일에 대한 갱신은 드물게 이루어진다
    - 동기화 오버헤드를 최소화할 수 있는 방법 요구
    - 낮은 응답 지연시간보다 높은 처리율이 더 중요하다
##### 3) GFS의 구성요소
|||
|:--:|:--|
|클라이언트|- 파일에 대한 읽기/쓰기 동작을 요청하는 애플리케이션으로 POSIX(Portable Operating System Interface) 인터페이스를 지원하지 않으며, 파일 시스템 인터페이스와 유사한 자체 인터페이스를 지원<br/>- 여러 클라이언트에서 원자적인 추가(atomic append)연산을 지원하기 위한 인터페이스를 지원|
|마스터|- 단일 마스터 구조로 파일 시스템의 이름 공간(name space), 파일과 chunk의 매핑 정보, 각 chunk가 저장된 청크서버들의 위치 정보 등에 해당하는 모든 메타 데이터를 메모리상에서 관리<br/>- 주기적으로 수집되는 청크서버의 하트비트 메시지를 이용하여 chunk들의 상태에 따라 chunk를 재복제하거나 재분산하는 것과 같은 회복동작을 수행<br/>- 하나의 chunk 서버를 primary로 지정하여 복제본의 갱신 연산을 일관되게 처리할 수 있도록 보장<br/>- 마스터에 대한 장애 처리와 회복을 위해 파일시스템 이름 공간과 파일의 chunk 매핑 변경 연산을 로깅하고, 마스터의 상태를 여러 섀도 마스터에 복제|
|청크서버|- 로컬 디스크에 chunk를 저장, 관리하면서 클라이언트로부터의 chunk 입출력 요청을 처리<br/>- 하트비트 메시지를 통해 청크서버의 상태에 대한 정보를 주기적으로 마스터에게 전달|
<hr>

##### 4) GFS에서 파일을 읽어오는 과정
![GFS](./GFS.png)
    - 클라이언트는 파일에 접근하기 위해 마스터로부터 해당 파일의 chunk가 저장된 chunk서버의 위치와 핸들을 먼저 받아온 뒤, 직접 청크서버에게 파일 데이터를 요청한다
#### 다. 하둡 분산 파일 시스템(HDFS, Hadoop Distributed File System)
##### 1) 개념 및 특징
    - HDFS는 GFS의 마스터와 유사한 하나의 네임노드(NameNode), GFS의 청크서버와 유사한 다수의 데이터노드(Datanode)로 구성
    - HDFS에서 파일 데이터는 블록단위로 나뉘어 여러 데이터노드에 분산,복제,저장
    - HDFS에서 기본적으로 파일은 한 번 쓰이면 변경되지 않는다고 가정(2.0 알파버전부터는 저장된 파일에 append가 가능)
    - HDFS는 순차적 스트리밍 방식으로 파일을 저장하거나 저장된 파일을 조회, 배치 작업에 적합하도록 설계
    - 낮은 데이터 접근 지연 시간보다는 높은 데이터 처리량에 중점
    - 클라이언트, 네임노드, 데이터노드 간의 통신을 위하여 TCP/IP 네트워크상에서 RPC를 사용
##### 2) HDFS의 구성요소
|||
|:--:|:--|
|네임노드|- 파일 시스템의 이름 공간 등 HDFS 상의 모든 메타데이터를 관리하며, 마스터/슬레이브 구조에서 마스터의 역할을 함<br/>- 파일이 어떤 형태의 블록 단위로 나누어져 있고, 어떤 노드에 특정 블록이 있는지 등 시스템 전반의 상태를 모니터링<br/> - 데이터를 저장하거나 애플리케이션을 실행하는 작업은 수행하지 않음<br/>- 클라이언트로부터의 파일 접근 요청을 처리<br/>- 데이터노드들로부터 하트비트를 받아 데이터노드들의 상태를 체크하는데, 하트비트 메시지에 포함된 블록정보를 가지고 블록의 상태를 체크할 수 있음|
|데이터노드|- HDFS의 슬레이브 노드로, 클라이언트로부터의 데이터 입출력 요청을 처리<br/>- 데이터의 유실을 방지하기 위해 블록을 3중 복제하여 저장<br/>- 블록을 저장할 때, 해당 블록에 대한 파일의 체크섬 정보를 별도로 저장<br/> - 주기적으로 데이터노드의 상태를 나타내는 하트비트와 자신이 관리하는 블록의 목록인 blockreport를 네임노드에게 전송|
|보조네임노드|- HDFS 상태 모니터링을 보조<br/>- 주기적으로 네임 노드의 파일 시스템 이미지를 스냅샷해 생성|
##### 3) HDFS의 파일 저장 과정
![hdfs-8-638](./hdfs-8-638.jpg)
    - 클라이언트는 저장할 파일을 여러 단위블록으로 분리
    - 블록을 저장하기 위한 데이터노드 주소를 받은뒤, 첫 번째 데이터노드에게 데이터를 전송
    - 첫 번째 데이터 노드는 전송받은 데이터를 저장한 후, 두 번째 데이터노드로 전달
    - 두 번째 데이터 노드도 전달받은 데이터를 저장한 후, 세 번째 데이터노드로 전달
    - 세 번째 데이터 노드까지의 데이터 저장이 완료되면, 각 데이터노드들은 순차적으로 클라이언트에게 저장이 완료되었다는 신호를 보냄
    - 이러한 과정을 모든 블록의 저장이 완료될 때까지 반복한다
    - 모든 블록의 저장이 완료되면 네임노드는 블록들이 저장된 데이터노드의 주소, 즉 파일에 대한 메타 데이터를 저장
##### 4) HDFS의 파일 읽기 과정
![hdfs-7-638](./hdfs-7-638.jpg)
    - 클라이언트는 읽고자 하는 파일에 대한 정보를 네임노드에게 요청
    - 네임노드는 파일에 대한 모든 블록의 목록과 블록이 저장된 데이터 노드의 위치를 클라이언트에게 반환
    - 클라이언트는 전달받은 블록의 위치를 이용해 데이터노드로부터 직접 데이터를 읽어 들인다

#### 라. 러스터(Lustre)
##### 1) 개념 및 특징
    - 객체 기반의 클러스터 파일 시스템
    - 클라이언트 파일 시스템, 메타데이터 서버, 객체 저장서버로 구성
    - 계층화된 모듈 구조 TCP/IP, 인피니밴드, 미리넷과 같은 네트워크 지원
##### 2) 구성요소
|||
|:--:|:--|
|클라이언트 파일 시스템|- 리눅스 VFS(Virtual File System)에서 설치할 수 있는 파일 시스템<br/>- 메타데이터 서버와 객체 저장 서버들과 통신하면서 클라이언트 응용에 파일 시스템 인터페이스 제공|
|메타데이터 서버|- 파일 시스템의 이름 공간과 파일에 대한 메타데이터를 관리|
|객체 저장 서버|- 파일데이터를 저장하고, 클라이언트로부터의 객체 입출력 요청을 처리<br/>- 데이터는 세그먼트라는 작은 데이터 단위로 분할해서 복수의 디스크 장치에 분산시키는 기술인 스트라이핑 방식으로 분산 저장|
##### 3) 구동방식
    - 러스터는 유닉스 시맨틱을 제공하면서 파일 메타데이터에 대해서는 라이트백 캐시를 지원
    - 클라이언트에서 메타데이터 변경에 대한 갱신 레코드를 생성하고 나중에 메타데이터 서버에 전달
    - 메타데이터 서버는 전달된 갱신 레코드를 재수행하여 변경된 메타데이터를 반영
    - 메타데이터 서버에서는 메타데이터를 동시에 접근하는 부하에 따라 클라이언트 캐시에서 라이트백 캐시를 지원하거나 메타데이터 서버에서 메타데이터를 처리하는 방식을 적용
    - 러스터는 메타데이터 서버에서 처리하도록 하는 방식을 사용해 메타데이터에 대한 동시 접근이 적으면 클라이언트 캐시를 이용한 라이트백 캐시를 사용하고, 메타데이터에 대한 동시접근이 많으면 클라이언트 캐시를 사용함으로써 발생할 수 있는 오버헤드를 줄인다.
    - 러스터는 파일의 메타데이터와 파일 데이터에 대한 동시성 제어를 위해 별도의 잠금을 사용한다.
    - 메타데이터에 접근하기 위해서는 메타데이터 서버의 잠금 서버로부터 잠금을 획득해야 한다.
    - 파일 데이터에 접근하기 위해서는 해당 데이터가 저장된 객체 저장 서버의 잠금 서버로부터 잠금을 획득해야 한다
    - 러스터는 클라이언트와 메타데이터 서버 간의 네트워크 트래픽을 최소화하기 위하여 메타데이터에 대한 잠금 요청 시에 메타데이터 접근 의도를 같이 전달하는 인텐트 기반 잠금 프로토콜을 사용한다
    - 메타데이터 서버는 메타데이터 접근 의도에 따라 해당 동작을 수행하고, 잠금을 승인하는 처리를 함꼐 수행함으로써 클라이언트와 메타데이터 서버간의 네트워크 트래픽을 줄일 수 있다. 
#### 마. 파일 시스템 비교
|구분|GFS|하둡 DFS|러스터|
|:--:|:--:|:--:|:--:|
|Open Source|O|O|O|
|Chunk Based|O|O|X|
|Support Replication|O|O|X|
|Multiple metadata server supported|X|X|X|
|Locks used to maintain atomicity|O|O|O|
|Uses a DB for storing metadata|X|X|X|
|Adding nodes without shutting down the system|O|O|O|
|POSIX support|X|X|O|
|Supports file modification|X|X|O|
<hr>

### 2. 데이터베이스 클러스터
#### 가. 개념
    - 하나의 데이터베이스를 여러 개의 서버상에 구축하는 것을 의미
    - 데이터베이스 파티셔닝은 데이터베이스를 여러 부분으로 분할하는 것
    - 분할된 각 요소는 파티션
    - 각 파티션은 여러 노드로 분할 배치되어 여러 사용자가 각 노드에서 트랜잭션을 수행
    - 데이터를 통합할 때, 성능과 가용성의 향상을 위해 데이터베이스 차원의 파티셔닝 또는 클러스터링을 이용
#### 나. 데이터베이스 파티셔닝 구현의 효과
|||
|:--:|:--|
|병렬처리|파티션 사이의 병렬 처리를 통한 빠른 데이터 검색 및 처리 성능을 얻을 수 있다|
|고가용성|특정 파티션에서 장애가 발생하더라도 서비스가 중단되지 않는다|
|성능향상|성능의 선형적인 증가 효과를 볼 수 있다|
#### 다. 데이터베이스 클러스터의 구분
##### 1) 무공유 디스크
    - 무공유 클러스터에서 각 데이터베이스 인스턴스는 자신이 관리하는 데이터 파일을 자신의 로컬 디스크에 저장하며, 이 파일들은 노드 간에 공유하지 않는다
    - 각 인스턴스나 노드는 완전히 분리된 데이터의 서브 집합에 대한 소유권을 가지고 있으며, 각 데이터는 소유권을 갖고 있는 인스턴스가 처리
    - 한 노드가 데이터 처리 요청을 받으면, 해당 노드는 처리할 데이터를 갖고 있는 노드에 신호를 보내 데이터 처리를 요청
    - Oracle RAC(Real Application Cluster)를 제외한 대부분의 데이터베이스 클러스터가 무공유 방식을 채택
|||
|:--:|:--|
|장점|노드 확장에 제한이 없다|
|단점|각 노드에 장애가 발생할 경우를 대비해 별도의 폴트톨러런스를 구성해야 한다|

##### 2) 공유 디스크
    - 공유 디스크 클러스터에서 데이터 파일은 논리적으로 모든 데이터베이스 인스턴스 노드들은 데이터 파일을 논리적으로 공유하며, 각 인스턴스는 모든 데이터에 접근할 수 있다.
    - 데이터를 공유하려면 SAN(Storage Area Network)과 같은 네트워크가 반드시 있어야 한다
    - 모든 노드가 데이터를 수정할 수 있기 때문에 노드간의 동기화 작업 수행을 위한 별도의 커뮤니케이션 채널이 필요하다
|||
|:--:|:--|
|장점|높은 수준의 폴트톨러런스를 제공하므로 클러스터를 구성하는 노드 중 하나의 노드만 살아 있어도 서비스가 가능하다 |
|단점|클러스터가 커지면 디스크 영역에서 병목현상이 발생|
#### 라. 데이터베이스 클러스터의 종류
##### 1) Oracle RAC 데이터베이스 서버
    - 공유 클러스터
    - 클러스터의 모든 노드에서 실행되며 데이터는 공유 스토리지에 저장
    - 특정 노드가 데이터를 소유하는 개념이 없다
    - 파티셔닝 필요 없다. 하지만, 성능향상을 위해 파티셔닝하는 경우 빈번
    - 응용 프로그램은 클러스터의 특정 노드가 아니라 RAC 클러스터에 연결
    - RAC는 클러스터의 모든 노드에 로드를 고르게 분산
<Oracle RAC 데이터베이스 서버의 장점>
|||
|:--:|:--|
|가용성|높은 수준의 폴트톨러런스를 제공하므로 클러스터를 구성하는 노드 중 하나의 노드만 살아 있어도 서비스가 가능하다|
|확장성|새 노드를 클러스터에 쉽게 추가할 수 있다.|
|비용 절감|예를들어 4CPU 16노드 클러스터를 사용하면 동급 성능의 64CPU SMP 시스템에 비해 비용을 크게 절감할 수 있다|

##### 2) IBM DB2 ICE(Integrated Cluster Environment)
    - CPU, 메모리, 디스크를 파티션별로 독립적으로 운영하는 무공유 방식의 클러스터링을 지원
    - 애플리케이션은 여러 파티션에 분산된 데이터베이스를 하나의 데이터베이스로 보게 되고, 데이터가 어느 파티션에 존재하고 있는지 알 필요가 없다.
    - 데이터와 사용자가 증가하면 애플리케이션의 수정없이 기존 시스템에 노드를 추가하고 데이터를 재분배함으로써 시스템의 성능과 용량을 일정하게 유지할 수 있다.
    - 각 노드로 분산되는 파티셔닝을 어떻게 구성하느냐에 따라 성능의 차이가 많이 발생할 수 있다.
    - 하나의 노드에 장애가 발생할 경우, 해당 노드에서 서비스하는 데이터에 대한 별도의 failover 메커니즘이 필요
    - DB2를 이용하여 클러스터링을 구성할 떄는 공유디스크 방식을 사용하여 가용성 보장
##### 3) 마이크로소프트 SQL Server
    - SQL Server는 연합 데이터베이스 형태로 여러 노드로 확장할 수 있는 기능을 제공
    - 연합 데이터베이스는 디스크 등을 공유하지 않는 독립된 서버에서 실행되는 서로 다른 데이터베이스들 간의 논리적인 결합이며, 네트워크를 이용하여 연결된다
    - 데이터는 관련된 서버들로 수평 분할되며, 테이블을 논리적으로 분리해 물리적으로는 분산된 각 노드에 생성한다
    - 각 노드의 데이터베이스 인스턴스 사이에 링크를 구성한 후 모든 파티션에 대해 UNION ALL을 이용해 논리적인 뷰를 구성하는 방식
    - 분산된 환경의 데이터에 대한 싱글 뷰를 제공
    - SQL Server에서는 이런 뷰를 DPV(Distributed Partitioned View)라고 한다
    - 마이크로소프트 SQL Server 구성의 문제점
        - DBA나 개발자가 파티셔닝 정책에 맞게 테이블과 뷰를 생성해야한다
        - 전역 스키마 정보가 없기 때문에 질의 수행을 위해 모든 노드를 액세스 해야 한다
        - 노드가 많아지거나 노드의 추가/삭제가 발생하는 경우 파티션을 새로 구성해야 한다. 또한 페일오버에 대해서는 별도로 구성해야 한다
    - SQL Server에서도 페일오버 메커니즘을 제공하지만, Active-Active가 아닌 Active-Standby 방법을 사용하고 있다
##### 4) MySQL
###### 가) 개념 및 특징
    - MySQL 클러스터는 비공유항으로서 메모리 기반 데이터베이스의 클러스터링을 지원
    - 병렬 서버구조로 확장이 가능
    - 관리노드, 데이터노드, MySQL 노드로 구성
|||
|:--:|:--|
|관리노드|클러스터를 관리하는 노드로 클러스터 시작과 재구성 시에만 관여|
|데이터노드|클러스터의 데이터를 저장하는 노드|
|MySQL|클러스터 데이터에 접근을 지원하는 노드|
    - 데이터의 가용성을 높이기 위해 데이터를 다른 노드에 복제시키며, 특정 노드에 장애가 발생하더라도 지속적인 데이터 서비스가 가능하다
    - 장애가 발생했던 노드가 복구되어 클러스터에 투입된 경우에도 기존 데이터와 변경돈 데이터에 대한 동기화 작업이 자동으로 수행
    - 데이터는 동기화 방식으로 복제되며, 이런 작업을 위해 일반적으로 데이터 노드 간에는 별도의 네트워크를 구성
    - MySQL의 최근 버전에서는 디스크 기반의 클러스터링을 제공한다.
    - 디스크 기반 클러스터링에서는 인덱스가 생성된 칼럼은 기존과 동일하게 메모리에 유지되지만, 인덱스를 생성하지 않은 칼럼은 디스크에 저장된다
###### 나) MySQL 클러스터 구성을 할 경우의 제한 사항
    - 파티셔닝은 LINEAR KEY 파티셔닝만 가능
    - 클러스터에 참여하는 노드(SQL 노드, 데이터노드, 매니저를 포함) 수는 255로 제한
    - 데이터 노드는 최대 48개까지만 가능
    - 트랜잭션 수행 중에 롤백을 지원하지 않으므로 작업 수행 중에 문제가 발생하였다면, 전체 트랜잭션 이전으로 롤백해야 한다
    - 하나의 트랜잭션에 많은 데이터를 처리하는 경우 메모리 부족 문제가 발생할 수 있으며 여러 개의 트랜잭션으로 분리해 처리하는 것이 좋다
    - 칼럼명의 길이는 31자, 데이터베이스의 테이블명 길이는 122자까지로 제한된다. 데이터 베이스 테이블, 시스템 테이블, 블롭 인덱스를 포함한 메타데이터는 2만 320개까지만 가능
    - 클러스터에서 생성할 수 있는 테이블 수는 최대 2만 320개
    - 한 로우의 크기는 8KB
    - 데이블의 키는 32개가 최대
    - 모든 클러스터의 기종은 동일해야 한다
    - 운영 중에 노드를 추가/삭제할 수 없다.
    - 디스크 기반 클러스터인 경우 tablespace의 개수는 2^32, tablespace당 데이터 파일의 개수는 2^16 데이터 파일의 크기는 32GB까지 가능


<hr>

### 3. NoSQL
#### 가. NoSQL의 개념 및 특징
    - 분산 데이터베이스 기술
    - 비관계형 데이터베이스 관리 시스템, SQL 계열 쿼리 언어 사용 가능
    - Not Only SQL이라고도 함
    - Key-value 모델, Document 모델(Jason 혹은 XML 데이터), Graph 모델, Column 모델
    - NoSQL은 key와 value의 형태로 자료를 저장하고, 빠르게 조회할 수 있는 자료 구조를 제공하는 데이터 저장소이다
    - 스키마 없이 동작하며, 구조에 대한 정의 변경 없이 자유롭게 데이터베이스의 레코드에 필드를 추가할 수 있따
    - 전통적인 RDBMS의 장점이라고 할 수 있는 복잡한 Join 연산 기능은 지원하지 않지만 대용량 데이터 처리와 대규모의 수평적 확장성을 제공한다
    - NoSQL은 대부분이 오픈소소
    - 구글 빅테이블, 아파치 HBase, 아마존 SimpleDB, 마이크로소프트 SSDS 등이 있다
#### 나. 구글 빅테이블
##### 1) 개념 및 특징    
    - 공유 디스크 방식
    - 모든 노드가 데이터, 인덱스 파일을 공유
    - 실시간 서비스뿐만 아니라 주기적인 배치 작업과 대용량 데이터의 분석 처리에도 적합하도록 구성
    - 빅테이블과 유사한 솔루션으로는 아파치 오픈소스 프로젝트인 HBase와 NHN가 개발한 Neptune이 있다
##### 2) 데이터 모델
    - 빅테이블은 multi-dimension sorted hash map을 파티션하여 분산 저장하는 저장소다.
    - 테이블 내의 모든 데이터는 row-key의 사전적 순서로 정렬, 저장
    - 테이블의 파티션은 row-key를 이용하여 이루어지고, Tablet이라고 불리는 분리된 파티션은 분산된 노드에서 서비스된다. 한 Tablet의 크기는 보통 100~200MB
    - row는 n개의 column-family를 가질 수 있으며 column-family에는 column-key, value, time stamp의 형태로 데이터를 저장한다.
    - 하나의 row-key, column-family내에 저장되는 데이터는 column-key의 사전적 순서로 정렬되어 있다.
    - 동일한 column-key에 대해 타임스탬프가 다른 여러 버전의 값이 존재할 수 있으며, 타임스탬프는 칼럼 값의 버전을 관리하기 위해 사용된다
    - BigTable에 저장되는 하나의 데이터의 키 값 또는 정렬 기준은 'rowkey + column-key + timestamp'이다
##### 3) 페일오버
    - 특정 노드에 장애가 발생할 경우, 빅테이블의 마스터는 장애가 발생한 노드에서 서비스되던 Tablet을 다른 노드로 재할당시킨다.
    - 재할당 받은 노드는 구글 파일 시스템에 저장도니 변경 로그 파일, 인덱스 파일, 데이터 파일 등을 이용해 데이터 서비스를 위한 초기화 작업을 수행한 후 다시 데이터 서비스를 한다
    - 빅테이블의 SPOF(Single Point Of Failure, 시스템의 구성 요소 중 동작하지 않으면 전체 시스템이 중단되는 요소)는 마스터
    - 분산 락 서비스를 제공하는 Chubby를 이용해 Master를 계속 모니터링 하다가 마스터에 장애가 발생하면 가용한 노드가 마스터 역할을 수행하도록 한다
    - Chubby는 폴트톨러런스 지원 구조이기 때문에 절대로 장애가 발생하지 않는다

![구글빅테이블](./구글빅테이블.jpg)

##### 4) AppEngine
    - AppEngine은 구글 클라우드 플랫폼의 일부로 빅테이블을 사용한다
    - 사용자에게 직접 빅테이블의 API를 공개하지 않고 추상 계층을 두고 있는데, API뿐만 아니라 데이터 모델도 추상화되어 있다
    - 사용자 테이블을 생성할 경우, 빅테이블의 테이블로 생성되는 것이 아니라 빅테이블의 특정 테이블에 대한 한 영역만을 차지하게 된다
    - 빅테이블에서는 별도의 사용자 정의 인덱스를 제공하지 않는 반면, AppEngine에서는 사용자가 수행하는 쿼리를 분석하여 자동으로 인덱스를 생성해 준다
    - AppEngine에서 생성한 인덱스도 빅테이블의 특정 테이블 또는 테이블 내의 컬럼으로 저장된다
#### 다. Hbase
    - 하둡 분산 파일 시스템을 기반으로 구현된 컬럼 기반의 분산 데이터베이스
    - HBase는 관계형 구조가 아니며, SQL을 지원하지 않는다
    - 구조화된 데이터보다는 비구조화된 데이터에 더 적합
    - 선형 확장이 가능
    - 수평적으로 확장성이 있어 큰 테이블에 적합하며, 단일행의 트랜잭션을 보장한다
    - 로우키에 대한 인덱싱만을 지원, Zookeeper를 이용한 고가용성 보장
    
#### 라. 아마존 SimpleDB
    - SimpleDB는 아마존의 데이터 서비스 플랫폼으로, 웹 애플리케이션에서 사용하는 데이터의 실시간 처리를 지원한다
    - SimpleDB는 주로 아마존의 다른 플랫폼 서비스와 같이 사용
    - EC2, S3등과 같은 아마존의 내부 서비스 간 네트워크 트래픽 무료, 외부와의 In/Out 트래픽에는 요금 부과
    - 사용자는 EC2에서 수행되는 웹 서버로 접근하고, 웹 서버에서 SimpleDB의 데이터를 조회해 적절하게 가공한 후 사용자에게 제공하는 형태로 구성
    - 비용을 염두에 두지 않은 경우라면 외부에서 직접 SimpleDB에 접근해 사용하는 것도 가능
    - SimpleDB는 하나의 데이터에 대해 여러 개의 복제본을 유지하는 방식으로 가용성을 높이며, 트랜잭션 종료 후 데이터가 모든 노드에 즉시 반영도지 않고 초 단위로 지연 동기화되는 'Eventual Consistency' 정책을 취한다
    - 관계형 데이터 모델과 표준 SQL을 지원하지 않으며, 전용 쿼리 언어를 이용
    - SimpleDB의 데이터 모델은 'Domain, Item, Attribute, Value로 구성되며 스키마가 없는 구조이다
|||
|:--:|:--|
|도메인|- 관계형 데이터베이스의 테이블과 동일한 개념<br/>- 도메인에는 최대 10GB의 데이터를 저장할 수 있으며, 사용자는 100개의 도메인을 가질 수 있음<br/>- 사용자는 최대 1,000GB의 데이터를 SimpleDB에 저장 가능|
|Item|- 관계형 데이터베이스의 레코드와 동일한 개념<br/>- item은 독립적인 객체를 나타내며, 1개 이상 256개 이하의 Attribute를 가짐|
|Attribute|- 관계형 데이터베이스의 칼럼과 동일한 개념이지만 사용하기 전에 미리 정의할 필요가 없음<br/>- Name, Value 쌍으로 데이터를 저장하고 저장되는 데이터의 Name은 attribute의 이름에 해당<br/>- item의 특정 Attribute(Cell)에는 여러 개의 값을 저장할 수 있음|
    - 여러 도메인에 걸친 쿼리는 허용되지 않는다.
    - 1:N관계의 데이터 모델을 갖는 두 개의 도메인으로부터 데이터를 조회할 경우 쿼리가 여러 번 수행돼야 하는 단점이 있다
|구분|내용|
|:--:|:--|
|CreateDomain|도메인을 생성|
|DeleteDomain|도메인을 삭제|
|ListDomains|모든 도메인의 목록을 가져옴|
|PutAttributes|Item을 생성하고 Attribute에 값을 추가|
|DeleteAttributes|Attribute 값을 삭제|
|GetAttributes|Attribute의 값을 조회|
|Query|-쿼리를 이용하여 조건에 맞는 여러개의 item을 조회<br/>- 한 번의 쿼리는 최대 5초 이내에 수행되어야하며, 쿼리 결과로 받을 수 있는 item 수는 256개|
#### 마. 마이크로소프트 SSDS
    - SSDS(SQL Server Data Service)의 데이터 모델은 컨테이너, 엔티티로 구성
|||
|:--:|:--|
|컨테이너|테이블과 유사한 개념이지만 하나의 컨테이너에 여러 종류의 엔티티를 저장할 수 있다|
|엔티티|엔티티는 레코드와 유사한 개념으로 하나의 엔티티는 여러 개의 property를 가질 수 있으며, property는 name-value 쌍으로 저장|
    - 관계형 데이터베이스에서는 엔티티를 구분하고 엔티티별로 테이블을 생성하는 것이 일반적이다. 하지만 SSDS를 이용하여 애플리케이션을 개발하면 관련된 정보를 하나의 컨테이너에 저장한다
    - SSDS는 컨테이너의 생성/삭제, 엔티티의 생성/삭제, 조회, 쿼리 등의 API를 제공하고 SOAP/REST 기반의 프로토콜을 지원
<hr>

## 2절. 분산 컴퓨팅 기술
### 학습목표
    - MapReduce의 개념, 특징, 실행과정을 이해한다
    - 병렬 쿼리 시스템의 다양한 종류와 그 특징들을 이해한다
    - SQL on Hadoop 기술과 종류를 이해한다
### 1. MapReduce
#### 가. MapReduce의 개념 및 특징
    - 분산 병렬 컴퓨팅 프레임워크
    - 분할정복 방식으로 대용량 데이터를 병렬로 처리
![wordcount_mapreduce_paradigm](./wordcount_mapreduce_paradigm.png)
    - 맵리듀스에서 Client의 수행 작업 단위는 맵리듀스 잡이라고 하며, 잡은 Map Task와 Reduce Task로 나뉘어서 실행된다
    - Map Task 하나가 1개의 블록을 대상으로 연산
#### 나. 구글 MapReduce
##### 1) 구글 MapReduce의 개발 배경
    - 연산의 병렬화, 장애 복구 등의 복잡성을 추상화시켜서 개발자들이 오직 핵심 기능 구현에만 집중할 수 있도록 하기 위해서 개발
##### 2) 프로그래밍 모델: Map과 Reduce라는 2개의 단계로 나눌 수 있음
![MapReduce_concept](./MapReduce_concept.png)
    - Map에서는 key와 value의 쌍들을 입력으로 받는다
    - 하나의 key, value쌍은 사용자가 정의한 Map 함수를 거치면서 다수의 새로운 key, value쌍들로 변환되어 로컬 파일 시스템에 저장
    - 저장된 임시 파일들은 프레임워크에 의해 Reduce에게 전송된다. 이 과정에서 자동으로 Shuffling과 Group by 정렬의 과정을 거친 후 Reduce의 입력 레코드로 들어가게 되는데 형식은 key와 value의 리스트다
    - Reduce의 입력 레코드들은 사용자가 정의한 Reduce 함수를 통해 최종 Output으로 산출된다
    - 사용자 관점에서는 이전에 언급했던 장애 복구와 같은 세세한 이슈들은 신경 쓸 필요없이 Map과 Reduce 두 함수만 작성하는 것만으로 대규모 병렬 연산 작업을 수행 할 수 있다.
##### 3) 실행 과정
    - 사용자가 MapReduce 프로그램을 작성해 실행한다
    - 마스터는 사용자의 프로그램에서 지정한 입력 데이터소스를 가지고 스케쥴링을 한다
    - 하나의 큰 파일은 여러 개의 파일 split들로 나뉘며, 각 split들이 Map 프로세스들의 할당 단위가 된다. split수만큼 Map Task들이 워커로부터 fork됨과 동시에 실행돼 Output을 로컬 파일 시스템에 저장 한다
    - 이 때, Output 값들은 partitioner라는 Reduce 번호를 할당해 주는 클래스를 통해 어떤 Reduce에게 보내질지 정해진다. 특별히 정해지지 않으면 Key와 해시값을 Reduce의 개수로 Modular 계산한 값이 부여되어 동일한 Key들은 같은 Reduce로 배정된다
    - Map 단계가 끝나면 원격의 Reduce 워커들이 자기에 할당된 Map의 중간 값들을 네트워크로 가져, 사용자의 Reduce 로직을 실행해 최종 산출물을 얻어낸다.
    - 보통 Reduce의 개수는 Map의 개수보다 적으며, 실행 흐름에서 알 수 있듯이 Map의 중간 데이터 사이즈에 따라 성능이 좌우된다
    - MapReduce 모델 적용의 적합성
<MapReduce 모델 적용의 적합성>
|적합한 경우|적합하지 않은 경우|
|:--:|:--:|
|분산 Grep이나 빈도 수 계산 등의 작업<br/>-> Map 단계를 거치면서 데이터 사이즈가 크게 줄어들고, 줄어든 크기만큼 Reduce 오버헤드도 줄어듦에 따라 성능상 이점이 많다. |정렬과 같은 작업<br/>-> 입력 데이터의 사이즈가 줄지않고, 그대로 Reduce로 전해지므로 오버헤드에 따라 수행 성능이 저하된다.|
##### 4) 폴트톨러런스
    - 각 프로세스에서는 Master에게 Task 진행 상태를 주기적으로 보낸다.
    - 마스터는 모든 워커들의 Task 상태 정보를 가지고 있다가 특정 워커의 태스크가 더 이상 진행되지 않거나 상태 정보를 일정한 시간 동안 받지 못하면 Task에 문제가 있다고 결론을 내린다
    - 이후 장애 복구를 해야 하는데 특정 Map이나 Reduce Task들이 죽은 경우, 해당 Task가 처리해야할 데이터 정보만 다른 워커에게 전해 주면 워커는 받은 데이터 정보를 인자로 새로운 Task를 재실행하면 된다
    - 이처럼 MapReduce는 Shared Nothing 아키텍처이기 떄문에 간단한 메커니즘을 가진다
#### 다. Hadoop MapReduce
##### 1) Haddop MapReduce의 개발 배경
##### 2) 아키텍처
    - 하둡은 데몬(서버의 메인메모리 상에서 백그라운드로 수행되는 프로그램)관점에서 4개의 구성요소를 가지고 있다.
|구분|설명|
|:--:|:--|
|네임노드|하둡을 이루는 가장 기본적이고 필수적인 데몬으로, 네임 스페이스를 관리하는 마스터 역할을 수행|
|데이터노드|분산 파일 시스템의 데몬으로 파일의 실질적인 데이터 입출력에 대한 처리를 수행|
|잡트래커|MapReduce 시스템에서 job이라는 작업을 관리하는 마스터에 해당(클러스터에 1개의 잡트래커가 존재)|
|태스크트래커|작업을 수행하는 워커 데몬이며 슬레이브에 해당(각 노드에 1개의 태스크 트래커가 존재|
    - 클라이언트에서 잡이라고 불리는 하둡 작업을 실행하면, 프로그램 바이너리와 입출력 디렉터리와 같은 환경 정보들이 JobTracker에게 전송된다
    - JobTracker는 작업을 다수의 Task로 쪼갠 후, 데이터 지역성을 보장하기 위해 그 Task들을 어떤 TaskTracker에게 보낼지를 감안해 내부적으로 스케줄링해 큐(Queue)에 저장한다
    - 이 때, Task는 맵퍼나 리듀서가 수행하는 작업 단위를 의미한다
    - TaskTracker는 JobTracker에게 3초에 한 번씩 주기적으로 하트비트를 보내 살아 있다는 것을 알린다.
    - TaskTracker에서 Heartbeat를 보내면 JobTracker는 먼저 해당 TaskTracker에게 할당된 Task가 있는지 큐에서 살펴본 후, Task가 있으면 하트비트의 Response 메시지에 Task 정보를 실어서 TaskTracker에게 보낸다.
    - TaskTracker는 Response 메시지의 내용을 분석해 프로세스를 fork해 자기에게 할당된 Task를 처리한다
##### 3) Hadoop MapReduce의 실행절차
![mapreduce_flow](mapreduce_flow.jpg)
    
    - MapReduce는 아래와 같은 단계들을 거쳐 실행된다.
        - 1. 스플릿 : HDFS의 대용량 입력 파일을 분리하여 파일스플릿을 생성하고, FileSplit 하나당 맵 태스크 하나씩을 생성
        - 2. 맵 : 각 Split에 대해서 레코드 단위로 map함수를 적용하여 key-value 쌍을 생성한다
        - 3. 컴바인 : 리듀스와 동일한 프로그램을 적용하여, 리듀스단계로 데이터를 보내기 전에 중간 결과값들을 처리하여 데이터의 크기를 줄여준다
        - 4. 파티션 : key를 기준으로 데이터를 디스크에 분할 저장하며, 각 파티션은 키를 기준으로 정렬이 수행된다. 또한 분할된 파일들은 각각 다른 리듀스 태스크에 저장된다. 
        - 5. 셔플 : 여러 맵퍼들의 결과 파일을 각 리듀서에 할당하고, 할당된 파일을 로컬 파일 시스템으로 복사한다.
        - 6. 정렬 : 병합 정렬방식을 이용하여 맵퍼의 결과파일을 key를 기준으로 정렬
        - 7. 리듀스 : 정렬 단계에서 생성된 파일에 대해 리듀스 함수를 적용한다
    - Hadoop Mapreduce 연산에서 기본적으로 Output format은 key와 value를 탭으로 구분하며, mapred.textoutputformat.separator 속성을 사용하여 구분자를 원하는 문자로 변경할 수도 있다.
    - 맵리듀스 작업의 대표적인 예제인 WordCount의 수행과정은 다음 도표와 같다.

![MapReduce-Way-MapReduce-Tutorial-Edureka](MapReduce-Way-MapReduce-Tutorial-Edureka.png)

##### 4) 하둡의 성능

    - MapReuce의 sort는 MapReduce에서 어떠한 작업을 실행하더라도 Map에서 Reduce로 넘어가는 과정에서 항상 발생하는 내부적인 프로세스이다
    - sort 작업은 데이터가 커질수록 처리 시간이 선형적으로 증가한다
    - 클러스터 구성 서버들의 숫자를 늘림으로써 처리 시간을 줄일 수 있는 것은 아니며, 플랫폼 자체적으로 선형 확작성을 갖고 있어야 처리 시간을 줄일 수 있다.
    - 이런 의미에서 sort는 하둡 같은 분산컴퓨팅 플랫폼의 성능과 확장성을 동시에 측정할 수 있는 좋은 실험이라고 할 수 있다. 

##### 5) 하둡 사용 현황

|구분|사용현황|
|:--:|:--|
|야후|- 활발하게 하둡을 사용하고 있는 하둡 프로젝트의 주요 후원자<br/>- 4만대 이상의 컴퓨터에서 하둡을 설치해 사용하고 있고, 가장 큰 클러스터는 약 4,500개의 노드로 구성|
|야후의 WebMap|- 야후의 대표적인 그래프 기반 검색 엔진<br/>- 알려진 웹 페이지들의 모든 edge 및 링크 정보를 계산해 그 결과를 다양한 검색 애플리케이션에서 사용할 수 있도록 해주는 거대한 그래프<br/>- 주기적으로 100개 이상의 MapReduce 작업들을 체인 형태로 묶어 실행시키는데, 출력결과만 압축해서 300TB 이상이 나올 정도로 대용량 데이터를 다루고 있음|
|국내|- NHN과 다음 등의 인터넷 포털에서 하둡을 사용 중<br/>- 삼성 SDS, SK 등의 IT회사에서 대용량 데이터 분석 등의 목적으로 하둡을 활용 중|
<hr>

### 2. 병렬 쿼리 시스템
#### 가. 병렬 쿼리 시스템의 개요
    - 구글이나 하둡의 MapReduce는 개발자들에게 구현하려는 알고리즘에만 포커싱 할 수 있도록 간단한 프로그래밍 모델을 제공하였으나, 일부 사용자들에게는 새로운 개념이기 떄문에 여전히 쉽지 않다.
    - 쿼리 인터페이스를 통해 병렬 처리를 할 수 있는 시스템들이 개발 됐다.
    - 구길의 Sawzall, 야후의 Pig 등이 있으며, 사용자가 MapReduce를 쉽게 사용할 수 있도록 새로운 쿼리 언어로 추상화된 시스템들이다
#### 나. 구글 Sawzall
    - 구글 Sawzall는 MapReduce를 추상화한 최초의 스크립트 형태 병렬 쿼리 언어다.
    - Pig, Hive도 Sawzall과 유사하다
#### 다. 아파치 Pig
##### 1) 정의 및 특징
    - 아파치 Pig는 야후에서 개발
##### 2) 개발 배경
    - 실제 대부분의 업무는 한번의 MapReduce 작업으로 끝나지 않는 경우가 많다.
##### 3) 사용 예제
    - MapReduce는 무공유 구조이기 떄문에 일반 RDB로는 쉽게 해결할 수 있는 join 연산을 매우 복잡하게 처리하므로 위 문제를 해결하기 위해 개발자는 약 400라인에 가까운 코드를 프로그래밍 해야 한다.
    - 하지만 Pig를 이용하면 단 10라인의 코드로 간단히 해결이 가능
##### 4) 사용 현황
#### 라. 아파치 하이브
##### 1) 정의 및 특징
    - 하이브는 페이스북에서 개발한 데이터 웨어하우징 인프라로 아파치 내의 하둡 서브 프로젝트로 등록돼 개발되고 있다.
    - SQL 기반의 쿼리 언어와 JDBC를 지원한다
    - Hadoop-Streaming을 쿼리 내부에 삽입해 사용할 수 있다.
    - 아파치 하이브는 맵리듀스의 모든 기능을 지원한다
##### 2) 개발배경
##### 3) 하이브 아키텍처
    - 하이브의 구성요소 중에서 MetaStore는 Raw File들의 콘텐츠를 일종의 테이블 내 칼럼처럼 구조화된 형태로 관리할 수 있게 해주는 스키마 저장소
    - 별도의 DBMS를 설정하지 않으면 Embedded Derby를 기본 데이터베이스로 사용
    - 앞 단에는 커맨드 라인 인터페이스가 있는데 사용자가는 이 CLI를 통해 Join이나 Group by 같은 SQL 쿼리를 사용
    - 그러면 파서에서 쿼리를 받아 구문 분석을 하고, MetaStore에서 테이블과 파티션 정보를 참조해 Execution Plan을 만든다
    - Execution Engine은 하둡의 JobTracker와 네임노드와 통신을 담당하는 창구 역할을 하면서 MapReduce 작업을 실행하고 파일을 관리한다
    - 아래 그림 오른쪾 하단의 SerDe라는 것은 Serializer와 Deserializer의 줄임말이며, 테이블의 로우나 칼럼의 구분자 등 저장 포맷을 정의해주는 컴포넌트다. 하둡의 InputForamt과 OutputFormat에 해당한다고 볼 수 있다.
##### 4) 하이브의 언어 모델

|DDL(Data Definition Language|DML(Data Manipulation Language|Query|
|:--|:--|:--|
|- 테이블 생성(Create table), 삭제(Drop Table), 변경(Rename Table) 명령<br/>- 테이블 스키마 변경(Alter Table, Add Column)<br/>- 테이블 조회(Show Table), 스키마 조회(Describe Table)|- 로컬에서 DFS로 데이터 로드(LOAD DATA)<br/>- 쿼리 결과를 테이블이나 로컬 파일시스템, DFS 저장|- Select, Group by, Sort by, Joins, Union, Sub Queries, Sampling, Transform|

<hr>

### 3. SQL on 하둡
#### 가. SQL on 하둡 개요
    - 실시간 제약사항을 극복하기 위한 시도 중 하나
    - 실시간 SQL 질의 분석 깃ㄹ
    - 임팔라에 대해 학습
#### 나. 임팔라의 개념 및 특징
    - SQL on 하둡 기술 중 먼저 대중에게 공개된 기술
    - 임팔라는 분석과 트랜잭션 처리를 모두 지원하는 것을 목표로 만들어진 SQL 질의 엔진
    - 하둡과 Hbase에 저장된 데이터를 대상으로 SQL 질의를 할 수 있다.
    - 고성능을 낼 수 있도록 C++언어를 사용, 맵리듀스를 사용하지 않고 실행 중에 최적화된 코드를 생성해 데이터를 처리
#### 다. 임팔라의 구성요소
|구분|설명|
|:--:|:--|
|클라이언트|ODBC/JDBC 클라이언트, 임팔라쉘 등에 해당하며, 임팔라에 접속해서 테이블 관리, 데이터 조회 등의 작업을 수행|
|메타스토어|임팔라로 분석할 대상 데이터들의 정보를 관리하며, 하이브의 메타데이터를 같이 사용|
|임팔라 데몬|시스템에서는 ImpalaD로 표시되며, 클라이언트의 SQL 질의를 받아서 데이터 파일들의 읽기/쓰기 작업을 수행한다. 임팔라 데몬은 질의 실행계획기, 질의 코디네이터, 질의 실행엔진으로 구성|
|스테이트 스토어|임팔라 데몬들의 상태를 체크하고 건강정보를 관리해주는 데몬으로, 임팔라 데몬에 장애가 생기면 다른 데몬들에게 이 사실을 알려서 이후부터는 장애가 발생한 데몬에게는 질의 요청이 가지 않도록 한다|
|스토리지|분석할 데이터의 저장소로 현재는 에이치베이스, 하둡 분산 파일 시스템의 두가지를 지원|

#### 라. 임팔라 동작 방식

![Impala](./Impala.png)

    - 모든 노드에 임팔라 데몬이 구동되며, 사용자는 이 데몬들이 구동된 임의의 노드에 JDB, ODBC, 임팔라쉘을 이용하여 질의를 요청할 수 있다.
    - 사용자의 질의는 데이터의 지역성을 고려해서 노드 전체로 분산되어 수행된다.
    - 사용자의 질의 요청을 받은 코디네이터 데몬은 분산되어 수행된 각 임팔라 노드들의 부분 결과들을 취합한 후 결과값을 만들어서 사용자에게 제공한다.
    - 실제 운영 환경에서는 라운드 로빈 방식으로 사용자 질의를 분산시켜서 질의에 대해 전 노드들이 코디네이터 역할을 고르게 수행할 수 있도록 해야 한다.
#### 마. 임팔라의 SQL 구문
    - 임팔라는 기본적으로 하이브의 SQL을 이용하지만 모든 하이브 SQL문을 지원하는 것은 아니기 때문에 어떤 구문이 지원되는지 확인할 필요가 있다.
|항목|설명|
|:--:|:--|
|데이터 정의 언어(Data Definition Language)|- 데이터베이스, 테이블 생성 : Create Database/Table<br/>- 테이블 변경, 파티션 추가 : Alter Table<br/>- 데이터베이스, 테이블 삭제 : Drop Database/Table<br/>- 데이터베이스 테이블 조회 : Show Database/Table, Describe Database|
|데이터 조작 언어(Data Manipulation Language)|- 데이터 조회 : Select, Where, GroupBy, OrderBy 구문 지원<br/>- 데이터 변경 구문은 지원 안함<br/>- 데이터 삭제(Delete)구문은 지원 안하나 테이블 삭제(Drop)시 데이터가 삭제됨|
|내장 함수(Builtin Functions)|- 수학함수 : 절대값(abs) 반환, 코사인값 반환(acos), 로그값 반환(log)등의 기능 제공<br/>- 타입 변환 : 날짜값 반환(day), 유닉스에 포타임 변환(from_unixtime), 현재 시간 반환(now) 등 다수의 함수 제공<br/>- 조건문 : if문제공, case 등 분기 기능 제공<br/> 문자열 함수 : 아스키 코드값 변환(ascii), 문자열 병합(concat)|

#### 바. 임팔라 데이터 모델
- 임팔라는 하둡 분산 파일시스템에 데이터를 저장하며, 어떤 저장포맷을 사용하느냐에 따라 데이터 조회시 처리 성능이 달라진다

|||
|:--:|:--|
|로우 단위로 저장|- 하둡의 기본 포맷인 텍스트나 시퀀스 파일은 로우 단위의 데이터 저장 방식을 사용한다<br/>- 테이블에서 하나의 칼럼을 읽든 전체 테이블을 읽든 동일한 디스크 입출력이 발생한다|
|칼럼 단위로 저장|- 읽고자하는 칼럼만큼의 디스크 입출력이 발생하기 때문에 처리 성능을 개선할 수 있다. -> 물론 전체 칼럼들을 모두 조회하는 질의는 저장 포맷에 의해 성능이 영향을 받지 않는다.<br/>- 로우 단위 파일 포맷을 사용했을 떄보다 처리 시간이 작게 걸리므로 처리 시간의 측면에서 더 효율적이다<br/>- 다만 하둡에 저장된 파일이 처음부터 칼럼 파일 포맷을 사용하지 않았을 경우, 파일 포맷 변경 작업을 해주어야 한다.<br/> 칼럼 단위의 파일 저장 포맷인 RCFile을 사용할 경우, 데이터 처리 과정에서 발생하는 디스크 입출력 양을 현저하게 줄일 수 있다.|
<hr>

## 3절. 클라우드 인프라 기술
### 학습목표
    - 서버 가상화의 개념과 그 효과를 이해한다
    - CPU 가상화 기술에 대해 이해한다.
    - 메모리 가상화 기술에 대해 이해한다.
### 1. 클라우드 컴퓨팅
#### 가. 클라우드 컴퓨팅의 개념 및 특징
    - 클라우드 컴퓨팅은 동적으로 확장 할 수 있는 가상화 자원들을 인터넷으로 서비스하는 기술을 의미한다
    - 이러한 클라우드 서비스들은 IaaS(Infrastructure as a Service), SaaS(Software as a Service), PaaS(Platform as a Service) 등 크게 3가지 유형으로 나뉜다.
    - VMware, Xen, KVM 등과 같은 서버 가상화 기술은 데이터센터나 기업들에게 인프라스트럭처를 위한 클라우드 서비스의 가능성을 보여주며, IaaS에 주로 활용
    - 아마존은 S3(Simpole Storage Service)와 EC2(Elastic Cloud Computing)환경을 제공함으로써 플랫폼을 위한 클라우드 서비스를 최초로 실현했다. 특히 AWS(Amazon Web Service)의 EMR(Electric MapReduce)은 하둡을 온디맨드로 이용할 수 있는 클라우드 서비스다.
    - 구글은 AppEngine, Apps, Gears, Gadgets 등을 제공함으로써 웹 기반의 다양한 소프트웨어들이 클라우드 서비스로서 어떻게 구체화될 수 있는지를 보여주었다
#### 나. 서버 가상화의 개념 및 특징
    - 인프라 기술은 클라우드 컴퓨팅의 근간이 되는 기술이며, 인프라 기술들 중에서도 가장 기반이 되는 기술은 서버 가상화
|구분|내용|
|:--:|:-|
|정의|물리적인 서버와 운영체제 사이에 적절한 계층을 추가해 서버를 사용하는 사용자에게 물리적인 자원은 숨기고 논리적인 자원만을 보여주는 기술이다|
|특징|- 서버 가상화는 하나의 서버에서 여러 개의 애플리케이션, 미들웨어, 운영체제들이 서로 영향을 미치지 않으면서 동시에 사용할 수 있도록 해준다.<br/>- 서버 가상화를 가능하게 하는 기술은 아주 다양하며 메인프레임, 유닉스 서버, x86 서버 등에 따라 서로 다른 기술이나 분류체계가 사용된다|

    - x86 계열 서버 군의 가장 큰 특징은 하드웨어, CPU, 운영체제의 공급업체가 모두 다르다.
    - 인텔, AMD 등과 같은 CPU 제공업체는 하드웨어 차원의 CPU 가상화를 주로 다루며, VMware나 마이크로소프트, 오픈소스, 커뮤니티에서는 소프트웨어 기반의 가상화 제품을 내놓고 있다.

#### 다. 서버 가상화 기술의 효과
    - 가상머신 사이의 데이터 보호
        - 하나의 물리적 서버에서 운영 중인 서로 다른 가상머신들 사이의 접속은 정상적인 네트워크 접속만을 허용한다. 가상머신은 보안상 서로 분리되어 데이터를 보호 받을 수 있다.
    - 예측하지 못한 장애로부터 보호
        - 가상머신에서 수행중인 애플리케이션의 장애가 다른 가상머신에는 전혀 영향을 미치지 않으며, 애플리케이션, 운영체제의 장애로부터 보호 받을 수 있다.
    - 공유 자원에 대한 강제 사용의 거부
        - 하나의 가상머신은 할당된 자원 이상을 가져가는 것을 차단할 수 있다. 이런 기능을 통해 다른 가상머신에 할당된 자원의 부족 현상을 차단할 수 있다. 예를 들어, 하나의 가상머신 I/O 병목 현상이 발생해도 다른 가상머신에서 I/O 병목 현상이 발생하지 않는다.
    - 서버 통합
        - 서비스, 데이터, 사용자 등의 증가로 더 많은 컴퓨팅 자원이 필요해졌지만 데이터 센터의 공간, 전원, 냉각장치는 제한적이다. 이런 문제를 해결하기 위해 기존 서버의 용량을 증설하고 가상머신을 추가함으로써 동일한 데이터센터의 물리적 자원을 이용하면서 더 많은 서버를 운영할 수 있다.
    - 자원 할당에 대한 증가된 유연성
        - 수시로 변화하는 각 가상머신의 자원 요구량에 맞추어 전체 시스템 자원을 재배치함으로써 자원 활용도를 극대화할 수 있다.
    - 테스팅
        - 다양한 운영체제나 운영환경에서 테스트가 필요한 경우, 새로운 서버를 추가하지 않아도 테스트 환경을 구성할 수 있다. 부하 테스트가 필요한 경우에도 일시적으로 자원을 줄이는 방법으로 부하 상황을 만들 수 있으며, 다수의 부하 생성 역할을 수행하는 노드도 쉽게 추가할 수 있다.
    - 정확하고 안전한 서버 사이징
        - 필요한 자원만큼 가상머신을 할당할 수 있으며, 사이징 예측이 불확실한 서버를 구성할 때에도 일단 확보된 리소스를 이용하여 할당한 후 쉽게 추가로 할당할 수 있다.
    - 시스템 관리
        - 마이그레이션 기능을 이용할 경우 운영 중인 가상머신의 중지없이 가상머신을 다른 물리적인 서버로 이동시킬 수 있다. 이런 기능을 이용하여 아래의 표 내용과 같은 업무를 수행할 수 있다.
    |||
    |:--:|:--|
    |하드웨어 장애|서버에 물리적으로 구성된 여러 디스크 중 1개의 디스크에 장애가 발생했을 때, 장애 발생 장비에서 운영되던 가상머신을 서비스 중지 없이 다른 장비로 이동시킨다. 그리고 장애가 발생한 장비의 디스크를 교체한 후 다시 서비스에 투입할 수 있다.|
    |로드 밸런싱|특정 가상 서버나 가상 서버가 수행중인 물리적인 서버에 부하가 집중되는 경우 여유있는 서버로 가상머신을 이동시킨다|
    |업그레이드|장비의 CPU 추가나 메모리 추가, 디스크 증설 등과 같은 작업이 필요한 경우, 다른 장비로 가상머신을 이동시킨 후 업그레이드 작업을 수행할 수 있다.|
<hr>

### 2. CPU 가상화
#### 가. 하이퍼바이저의 개념 및 특징
    - 물리적 서버 위에 존재하는 가상화 레이어를 통해 운영체제를 수행하는데 필요한 하드웨어 환경을 가상으로 만들어준다
    - 하이퍼바이저는 호스트 컴퓨터에서 다수의 운영 체제를 동시에 실행하도록 하기 위한 논리적인 플랫폼을 의미한다.
    - 엄격하게 구분할 경우에는 차이가 있지만 일반적으로 가상머신을 하이퍼바이저라고 할 수 있으며, 하이퍼바이저는 VMM(Virtual Machine Monitor)이라고도 한다.
    - 하이퍼바이저는 서버 가상화 기술의 핵심으로 x86 계열 서버 가상화에서는 소프트웨어 기반으로 하이퍼바이저를 구성한다
    - 하이퍼바이저를 통해 사용자는 추가 하드웨어 구입 없이 새로운 운영체제의 설치, 애플리케이션의 테스팅 및 업그레이드를 동일한 물리적 서버에서 동시에 수행할 수 있다.
#### 나. 하이퍼바이저의 기능
    - 하드웨어 환경 에뮬레이션
    - 실행환경 격리
    - 시스템 자원 할당
    - 소프트웨어 스택 보존
#### 다. 하이퍼바이저 관련 기술의 분류
##### 1) 플랫폼별 분류
|x86 계열|유닉스 계열|메인 프레임 계열|
|:--:|:--:|:--:|
|VMware, MS virtual Server, Xen|IBM의 POWER hypervisor 등|z/VM 하드웨어 펌웨어로 분류되는 PR/SM 등|
##### 2) 하이퍼바이저의 위치와 기능에 따른 분류
    - 가상화를 제공하는 하이퍼바이저가 물리적인 하드웨어 또는 호스트 운영체제와의 관계에서 어디에 위치하는지에 따라 베어메탈 하이퍼바이저와 호스트 기반 하이퍼바이저로 나뉠 수 있다
    - 베어메탈 하이퍼바이저는 하드웨어와 호스트 운영체제 사이에 위치하며, 호스트 기반 하이퍼바이저는 호스트 운영체제와 게스트 운영체제 사이에 위치한다.
    - 베어메탈 하이퍼바이저는 다시 반가상화와 완전가상화로 구분할 수 있다.
##### 3) privileged 명령어 처리 방법에 따른 분류
    - 최근에는 하이퍼바이저를 제공하는 소프트웨어 벤더들이 다양한 가상화 기법을 도입하고 있으며, CPU 제조업체에서도 하드웨어에서 가상화 기술을 지원하는 등 새로운 가상화 방법이 계속 나오고 있기 때문에 서버 가상화 기술을 정확하게 분류하기는 힘들다.
    - x86 계열 운영체제는 자신의 모든 하드웨어에 대한 제어 소유권을 갖고 있다는 가정 아래 하드웨어에 직접명령을 수행하는 방식으로 디자인돼 있다.
    - x86 아키텍처는 하드웨어에 대한 접근 권한을 관리하기 위해 Ring 0,1,2,3 등 4개의 레벨로 구성돼 있다. 일반적으로 사용자 애플리케이션 Ring 3레벨로 수행되며, 운영체제의 경우 메모리나 하드웨어에 직접 접근해야 하기 때문에 Ring 0 레벨에서 수행 된다.
![2-physical-server-rings-and-levels](2-physical-server-rings-and-levels.jpg)
    
    - 가상머신 내에서도 운영체제가 필요하고 이 운영체제는 Ring 0의 권한을 필요하게 된다. 가상머신의 운영체제가 응용 애플리케이션 권한(Ring 3)으로 수행될 경우 x86 아키텍처에서는 복잡한 문제가 발생한다.
    - Ring 3에서 수행된 가상머신 운영체제에서 Ring 0 수준의 명령을 호출하면 가상화를 지원하는 계층에서 이를 Ring 0 수준의 명령어로 다시 변환해 실행해야 하며, 이를 위해 가상화지원 계층은 반드시 Ring 0 레벨로 수행되어야 한다.
    - x86 아키텍처에서 가상화 기술의 핵심은 가상머신이 요청하는 privileged 명령어를 어떻게, 어떤 계층에서 처리 하느냐이다. 가상화의 용어 중 완전 가상화, 반가상화라는 용어도 privileged 명령어를 어떻게 처리하느냐를 기준으로 분류한 것
#### 라. 가상화 방식의 분류
##### 1) 완전 가상화
    - 하이퍼바이저보다 우선순위가 낮은 가상머신에서는 실행되지 않는 privileged 명령어에 대해서 trap을 발생시켜 하이퍼바이저에서 실행하는 방식이다.
    - VMware ESX Server, MS Virtual Server 등의 제품이 완전 가상화 기반 솔루션이다.
    - 초기 Xen에서는 완전 가상화를 지원하지 않았지만, 최근 Intel VT-x, AMD-V 환경에서 완전 가상화를 지원하고 있다.
|||
|:--:|:--|
|장점|- CPU뿐만 아니라 메모리, 네트워크 장치 등 모든 자원을 하이퍼바이저가 직접 제어,관리 하기 떄문에 어떤 운영체제라도 수정하지 않고 설치가 가능<br/>- MS 윈도우와 같은 Guest OS가 하이퍼바이저 상에서 변경되지 않은 상태로 실행될 수 있음|
|단점|- 하이퍼바이저가 자원을 직접 제어하기 떄문에 성능에 영향을 미침<br/>- 자원들이 하이퍼바이저에 너무 밀접하게 연관돼있어 운영 중인 게스트 운영체제에 할당된 CPU나 메모리 등의 자원에 대한 동적변경 작업이 단일 서버 내에서는 어려움<br/>- 자원에 대한 동적변경을 하기 위해서는 VMware의 VMotion과 같은 솔루션의 도움을 받아야 함<br/>- Para Virtualization에 비해 속도가 느림|
##### 2) 하드웨어 지원 완전 가상화
    - 최근에는 완전 가상화 방식에서 Intel VT-x, AMD-V CPU의 하드웨어에서 제공하는 가상화 기능을 이용하고 있다.
    - 가상머신에서 메모리와 CPU 등의 하드웨어에 명령을 내릴 수 있는 반가상화 수준의 성능을 발휘하도록 개선하고 있다.
![VMware](./VMware.jpg)

    - CPU에 Ring -1 계층이 추가되었으며, 하이퍼바이저는 Ring -1에서 수행되고 가상머신의 운영체제는 Ring 0에서 수행되어 privileged 명령어에 대해 추가로변환 과정이 필요 없다
    - 하이퍼바이저를 거쳐 바로 하드웨어로 명령이 전달돼 빠른 성능을 보장
    - 윈도우 2008 서버의 Hyper-V는 반드시 가상화 지원 CPU만을 사용해야 한다
    - 인텔에서 제시하는 하드웨어 지원 가상화 사용의 주의점: "하드웨어 지원 가상화를 사용하는 경우 CPU 사용률이 높아진다. 특히 I/O나 메모리를 많이 사용하는 경우 CPU 사용률이 높아진다. 따라서 서버 통합을 목적으로 하는 경우 비효율적일 수도 있다."
    - 인텔에서는 반가상화와 하드웨어 지원 완전 가상화를 모두 사용하는 하이브리드 가상화를 제시하고 있다.
    - Xen을 이용한 하이브리드 가상화의 경우, 반가상화용으로 수정된 운영체제에 하드웨어 지원 완전 가상화 모듈을 탑재해 명령어의 종류에 따라 반가상화 또는 완전 가상화를 선택, 사용하도록 한다
##### 3) 반가상화
##### 4) Monolithic vs Microkernel
##### 5) 호스트 기반 가상화
##### 6) 컨테이너 기반 가상화
<hr>

### 3. 메모리 가상화
#### 가. 개념 및 특징
#### 나. 가상머신 메모리 할당
#### 다. 가상머신 메모리 할당의 문제 해결을 위한 방법
<hr>

### 4. I/O 가상화
#### 가. 가상 이더넷
#### 나. 공유 이더넷 어댑터
#### 다. 가상 디스크 어댑터
